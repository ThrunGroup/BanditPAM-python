% !TEX root = 0-main.tex

\section{Preliminaries \label{sec:prelims}}
For $n$ data points $\mathcal{X} =  \{ x_1, x_2, \cdots, x_n \}$ and a user-specified distance function $d(\cdot, \cdot)$, the $k$-medoids problem aims to find a set of $k$ medoids $\mathcal{M} = \{m_1, \cdots, m_k \} \subset \mathcal{X}$ to minimize the overall distance of points from their closest medoids:
\begin{equation} \label{eqn:total-loss}
	L(\mathcal{M}) =  \sum_{i=1}^n \min_{m \in \mathcal{M}} d(m, x_i)
\end{equation}
Note that $d$ does not need to satisfy symmetry, triangle inequality, or positivity. 
For the rest of the paper, we use $[n]$ to denote the set $\{1,\cdots,n\}$ and $\vert \mathcal{S} \vert$ to represent the cardinality of a set $\mathcal{S}$.
For two scalars $a,b$, we let $a\wedge b = \min(a,b)$ and $a\vee b = \max(a,b)$.

\subsection{Partitioning Around Medoids (PAM)}
The original PAM algorithm \cite{kaufman1987clustering, kaufman1990partitioning} first initializes the set of $k$ medoids via the BUILD step and then repeatedly performs the SWAP step to improve the loss \eqref{eqn:total-loss} until convergence.

\paragraph{BUILD:} PAM initializes a set of $k$ medoids by greedily assigning medoids one-by-one so as to minimize the overall loss \eqref{eqn:total-loss}. 
The first point added in this manner is the medoid of all $n$ points.
Given the current set of $l$ medoids $\mathcal{M}_{l} = \{m_1, \cdots, m_{l}\}$, the next point to add $m^*$ can be written as
\begin{equation}
\label{eqn:build-next-medoid}
    \text{BUILD:~~~~}m^* = \argmin_{x \in \mathcal{X} \setminus \mathcal{M}_{l}} \frac{1}{n} \sum_{j=1}^n \left[d(x, x_j) \wedge \min_{m' \in \mathcal{M}_{l}} d(m', x_j)\right]. 
\end{equation}
\paragraph{SWAP:} PAM then swaps the medoid-nonmedoid pair that would reduce the loss \eqref{eqn:total-loss} the most among all possible $k(n-k)$  such pairs.
Let $\mathcal{M}$ be the current set of $k$ medoids. Then the best pair to swap is
\begin{equation}
\label{eqn:next-swap}
    \text{SWAP:~~~~}(m^*, x^*) = \argmin_{(m,x) \in \mathcal{M} \times (\mathcal{X} \setminus \mathcal{M}) } \frac{1}{n} \sum_{j=1}^n \left[d(x, x_j) \wedge \min_{m' \in \mathcal{M}\setminus \{m\}} d(m', x_j) \right]. 
\end{equation}
The second term in both \eqref{eqn:build-next-medoid} and \eqref{eqn:next-swap}, namely $\min_{m' \in \mathcal{M}_{l}} d(m', x_j)$ and $\min_{m' \in \mathcal{M}\setminus \{m\}} d(m', x_j)$, can be determined by caching the smallest and the second smallest distances from each point to the previous set of medoids, namely $\mathcal{M}_{l}$ in \eqref{eqn:build-next-medoid} and $\mathcal{M}$ in \eqref{eqn:next-swap}.
Therefore, in both \eqref{eqn:build-next-medoid} and \eqref{eqn:next-swap}, we only need to compute the distance once for each summand.
As a result, PAM needs $O(kn^2)$ distance computations for the $k$ greedy searches in the entire BUILD step and $O(kn^2)$ distance computations for each SWAP iteration.


% The second term in \eqref{eqn:build-next-medoid}, namely $\min_{m' \in \mathcal{M}_{l}} d(m', x_j)$, can be determined by caching the smallest distance from each point to the previous set of medoids $\mathcal{M}_{l}$.
% Similarly, the second term \eqref{eqn:next-swap}, namely $\min_{m' \in \mathcal{M}\setminus \{m\}} d(m', x_j)$, can be determined by caching the smallest and the second smallest distances from each point to the previous set of medoids $\mathcal{M}$.
% Therefore, in for both \eqref{eqn:build-next-medoid} and \eqref{eqn:next-swap}, we only need to compute the distance once for each term in the summation.
% As a result, the time complexity of PAM is $O(kn^2)$ for $k$ greedy searches in the entire BUILD subroutine and $O(kn^2)$ for each SWAP iteration. We refer to the sequence of medoids selected by the BUILD step and the swaps performed by the SWAP step as the optimization trajectory of PAM.