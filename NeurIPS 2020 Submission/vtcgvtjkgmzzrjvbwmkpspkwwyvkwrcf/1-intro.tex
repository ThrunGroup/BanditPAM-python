% !TEX root = 0-main.tex

\section{Introduction \label{intro}}

Many modern data science applications require the clustering of very-large-scale data. 
Due to its computational efficiency, the $k$-means clustering algorithm \cite{macqueen1967some,lloyd1982least} has been one of the most widely-used clustering algorithms.
$k$-means alternates between assigning points to their nearest cluster centers and recomputing those centers. 
Central to its success is the specific choice of the cluster center: for a set of points, $k$-means defines the cluster center as the point with the smallest average \emph{squared Euclidean distance} to all other points in the set. 
Under such a definition, the cluster center is the arithmetic mean of the cluster's points and can be computed efficiently. 

While commonly used in practice, $k$-means clustering suffers from several drawbacks. 
First, while one can efficiently compute the cluster centers under squared Euclidean distance, it is not straightforward to generalize to other distance metrics \cite{overton1983quadratically,jain1988algorithms,bradley1997clustering}. 
However, a different distance may be desirable in different applications.
% In different applications, other distance metrics may be desirable.
For example, $l_1$ and cosine distance are often used in sparse data, such as in recommendation systems \cite{leskovec2020mining} and single-cell RNA-seq analysis \cite{ntranos2016fast}; additional examples include string edit distance in text data \cite{navarro2001guided}, and graph metrics in social network data \cite{mishra2007clustering}. 
Second, the cluster center in $k$-means clustering is in general not a point in the dataset and may not be interpretable in many applications. This is especially problematic when the data is structured, such as parse trees in context-free grammars, sparse data in recommendation systems \cite{leskovec2020mining}, or images in computer vision where the mean image is visually random noise \cite{leskovec2020mining}. 

Alternatively, $k$-medoids clustering algorithms \cite{kaufman1987clustering,kaufman1990partitioning} use the \emph{medoid} to define the cluster center for a set of points, where for an arbitrary distance function, the medoid is the point \emph{in the set} that minimizes the average distance to all the other points.
Note that the distance metric can be arbitrary---indeed, it need not be a distance metric at all and could be an asymmetric dissimilarity measure---which addresses the first shortcoming of $k$-means outlined above.
Moreover, unlike $k$-means, the cluster centers in $k$-medoids, i.e. the medoids, are restricted to be points in the dataset, thus addressing the second shortcoming of $k$-means clustering described above.

% \mo{Need to make point that until now, PAM is $O(n^2)$, or you have to settle for suboptimal loss}
% \mo{Also need to make the point that $k$-medoids has been largely unpopular due to its computational inefficiency}

Despite its advantages, $k$-medoids clustering is less popular than $k$-means due to its computational efficiency. 
Indeed, state-of-art $k$-medoids clustering algorithms are iterative and are quadratic in the data size, whereas $k$-means is linear in dataset size in each iteration. 

Mathematically, for $n$ data points $\mathcal{X} =  \{ x_1, x_2, \cdots, x_n \}$ and a user-specified distance function $d(\cdot, \cdot)$, the $k$-medoids problem is to find a set of $k$ medoids $\mathcal{M} = \{m_1, \cdots, m_k \} \subset \mathcal{X}$ to minimize the overall distance of points from their closest medoids: 
\begin{equation} \label{eqn:sec_intro_total-loss}
    L(\mathcal{M}) =  \sum_{i=1}^n \min_{m \in \mathcal{M}} d(m, x_i)
\end{equation}
This problem is, unfortunately, NP-hard in general \cite{schubert2019faster}.
Partitioning Around Medoids (PAM) \cite{kaufman1987clustering, kaufman1990partitioning} is one of the most widely used heuristic algorithms for $k$-medoids clustering.
PAM is split into two subroutines: BUILD and SWAP.
First, in the BUILD step, PAM aims to find an initial set of $k$ medoids by greedily and iteratively selecting points that minimize the $k$-medoids clustering loss \eqref{eqn:sec_intro_total-loss}. 
Next, in the SWAP step, PAM considers all $k(n-k)$ possible pairs of medoid and non-medoid points and swaps the pair that reduces the loss the most. 
The SWAP step is repeated until no further improvements can be made with more swaps.

PAM has been empirically shown to produce better results than other popular $k$-medoids clustering algorithms \cite{reynolds2006clustering,schubert2019faster}.
However, the BUILD step and each of the SWAP steps require  $O(kn^2)$ distance evaluations and can be prohibitively expensive to run, especially for large datasets or when the distance evaluations are themselves expensive (e.g. edit distance between two long strings).
% \martin{I removed the detailed complexity description because I have discussed them in the preliminary section, and I see no need for readers to learn it at this point.}

Randomized algorithms like CLARA \cite{kaufman1990partitioning} and CLARANS \cite{ng2002clarans} have been proposed to improve computational efficiency, but at the cost of deteriorated clustering quality.
More recently, Schubert et al. \cite{schubert2019faster} proposed a deterministic algorithm, dubbed FastPAM1, that guarantees the same output as PAM but improves the complexity to $O(n^2)$ when the cluster sizes are similar. However, the factor $O(k)$ improvement becomes less relevant when the sample size $n$ is large and the number of medoids $k$ is small compared to $n$.  Throughout the rest of this work, we treat $k$ fixed and assume $k \ll n$.

% important for large values of $k$.

% Apart from Section \ref{subsec:algdetails}, where we describe the FastPAM1 algorithm, we treat $k$ as fixed and assume $k \ll n$.
%\martin{What happens to the FastPAM trick?}

% PAM has been empirically shown to produce better results than other popular $k$-medoids clustering algorithms \cite{reynolds2006clustering,schubert2019faster}. However, it can be prohibitively expensive to run, especially for large datasets. 
% The BUILD subroutine takes $O(n^2)$ distance evaluations to initialize each of the $k$ medoids, resulting in a complexity of $O(kn^2)$
% Each SWAP iteration also takes $O(kn^2)$ distance evaluations, in order to compute the change in loss for each of the $k(n-k)$ medoid-and-non-medoid pairs with respect to the remaining $n-k$ non-medoid points. 
% Indeed, most of the runtime of PAM is spent in distance evaluations \{citation\}, which suggests it is desirable to lower this complexity.

% Randomized algorithms like CLARA \cite{kaufman1990partitioning} and CLARANS \cite{ng2002clarans} have been proposed to improve computational efficiency, but at the cost of deteriorated clustering quality.
% More recently, Schubert et al.~proposed a deterministic algorithm FastPAM1 \cite{schubert2019faster} that guarantees the same output as PAM but improves the complexity to $O(n^2)$ when the cluster sizes are similar. The factor $O(k)$ improvement is particularly important for large values of $k$. Apart from Section \ref{subsec:algdetails}, where we describe the FastPAM1 algorithm, we treat $k$ as fixed and assume $k \ll n$.
%\martin{What happens to the FastPAM trick?}

\paragraph{Contributions:} 
In this work, we propose a novel randomized $k$-medoids algorithm, called \algnamenospace, that significantly improves the computational efficiency of PAM while returning the same result with high probability.
We theoretically prove that \algname reduces the complexity on the sample size $n$ from $O(n^2)$ to $O(n\log n)$, both for the BUILD step and each SWAP step, under reasonable assumptions that hold in many real-world datasets.
% As a side note, for large $k$, the FastPAM1 $O(k)$ improvement technique \cite{schubert2019faster} is also compatible with \algnamenospace. 
We empirically validate our results on several large-scale real-world datasets
%with various structures, including the Hour of Code \#4 coding exercise submissions dataset from Code.org, the 10x Genomics 68k PBMC single-cell RNA sequencing dataset, and the MNIST handwritten digits dataset.
and observe that \algname provides a reduction of distance evaluations of up to 200x while returning the same result as PAM.
We also release a high-performance C++ implementation of \algnamenospace, which brings a 3.2x wall-clock-time speedup over the state-of-the-art FastPAM implementation \cite{schubert2019elki} on the full MNIST dataset without precomputing and caching the $O(n^2)$ pairwise distances as in prior state-of-the-art approaches.

% Overall, \algname enables $k$-medoids clustering on a wide range of applications, including identifying cell types in large-scale single-cell data or providing scalable feedback for students learning computer science.
% \martin{I substantially changed the above paragraphs and please take a look}

% We propose \algnamenospace, a randomized algorithm inspired by techniques from multi-armed bandits, that greatly improves the computational efficiency of PAM.
% We theoretically prove that \algname reduces the complexity of each PAM iteration from $O(n^2)$ to $O(n\log n)$, while guaranteeing to return the same results with high probability. 
% We empirically validate our results on several large-scale real-world datasets with various structures, including the HOC4 coding exercise submissions dataset, the 10X Genomics 68k PBMC single-cell RNA sequencing dataset, and the MNIST handwritten digits dataset.
% We observe that \algname provides up to 100x reduction of distance computations while returning the same result as PAM. 
% Such an improvement enables $k$-medoids clustering on a wide range of applications including identifying cell types in large-scale single-cell data or providing scalable feedback for students learning computer science.
% \martin{I substantially changed this paragraph and please take a look}

% Crucially, \algname takes $O(n \log n)$ distance evaluations in each iteration, effectively improving each step from quadratic to almost linear. On MNIST, the state-of-the-art implementation of FastPAM caches the pairwise distance matrix and runs in approximately 3 hours; \algname runs in less than an hour and returns the same results. Furthermore, \algname does not cache the pairwise distance matrix, resulting in an $O(n^2)$ savings in space.


Intuitively, \algname works by recasting each step of PAM from a \emph{deterministic computational problem} to a \emph{statistical estimation problem}. 
In the BUILD step assignment of the $l$th medoid, for example, we need to choose the point amongst all $n-l$ non-medoids that will lead to the lowest overall loss \eqref{eqn:sec_intro_total-loss} if chosen as the next medoid. Mathematically, we wish to find $x$ that minimizes:
\begin{equation} \label{eqn:sec_intro_build_loss}
	L(x; \mathcal{M}) = \sum_{j=1}^n \min_{m \in \mathcal{M} \cup \{x\}} d(m, x_j) \overset{\text{def}}{=} \sum_{j=1}^n g(x_j),
\end{equation}
where $g(\cdot)$ is a function that depends on $\mathcal{M}$ and $x$.
Eq.~\eqref{eqn:sec_intro_build_loss} shows that the loss of a new medoid assignment $L(\mathcal{M}; x)$ can be written as the summation of the value of the function $g(\cdot)$ evaluated on all $n$ points in the dataset. Though approaches such as PAM compute $L(\mathcal{M}; x)$ exactly for each $x$, \algname \textit{adaptively estimates} this quantity by sampling reference points $x_j$ for the most promising candidates. Indeed, computing $L(\mathcal{M}; x)$ exactly for every $x$ is not required; promising candidates can be estimated with higher accuracy (more reference point $x_j$'s) and less promising ones can be discarded early without requiring further unnecessary computation.

To design the adaptive sampling strategy, we show that the BUILD step and each SWAP iteration can be formulated as a best-arm identification problem from the multi-armed bandits (MAB) literature \cite{audibert2010best,even2002pac,jamieson2014lil,jamieson2014best}. 
In the typical version of the best-arm identification problem, we have $m$ arms. At each time step $t = 0,1,\cdots,$ we decide to pull an arm $A_t\in \{1,\cdots,m\}$, and receive a reward $R_t$ with $E[R_t] = \mu_{A_t}$. The goal is to identify the arm with the largest expected reward with high probability while expending the fewest number of total arm pulls.
In the BUILD step, we view each candidate medoid $x$ as an arm in a best-arm identification problem. The arm parameter corresponds $E[g]$, and by pulling an arm, we observe the loss evaluated on a randomly sampled data point $x_j$. Using this reduction, the best candidate medoid can be estimated using existing best-arm algorithms like the Upper Confidence Bound (UCB) algorithm \cite{lai1985asymptotically}.

% \subsection{Related work}
% \mo{RAND and other baselines in Meddit paper (TOPRANK?) necessary to mention?}

% The $k$-means clustering algorithm is one of the \emph{centroid}-based clustering methods, where the centroid of a set of points for a distance metric is defined as the point \emph{in the space}
% % \mo{"among the datapoints"?}
% that minimizes the overall distance to other points.
% %So the key difference between centroid and medoid is that a centroid does not need to be one of the points in the set. 
% $k$-means uses the centroid for squared Euclidean distance to define the cluster center, which is the arithmetic mean and can be computed in linear time. 
% Centroid for Bregman divergence is also the arithmetic mean and centroid for $l_1$ distance is the component-wise median; the latter is used by the $k$-medians clustering algorithm \cite{bradley1997clustering,jain1988algorithms}.
% However, the centroid for other distances is in general hard to compute.
% For example, the centroid for the unsquared Euclidean distance corresponds to the much harder Weber problem with no closed-form solution \cite{bradley1997clustering,overton1983quadratically}. 

\paragraph{Related work:} Many other $k$-medoids algorithms exist, in addition to CLARA, CLARANS, and FastPAM as described above. Park et al.~\cite{park2009simple} proposed a $k$-means-like algorithm that alternates between reassigning the points to their closest medoid and recomputing the medoid for each cluster until the $k$-medoids clustering loss can no longer be improved. 
Other proposals include optimizations for Euclidean space and tabu search heuristics \cite{estivill2001robust}. Recent work has also focused on distributed PAM, where the dataset cannot fit on one machine \cite{song2017pamae}. All of these algorithms, however, scale quadratically in dataset size or concede the final clustering quality for improvements in runtime.

The idea of algorithm acceleration by converting a computational problem into a statistical estimation problem and designing the adaptive sampling procedure via multi-armed bandits has
witnessed a few recent successes \cite{chang2005adaptive,kocsis2006bandit,li2016hyperband,jamieson2016non,bagaria2018adaptive,zhang2019adaptive}.
% , such as in Monte Carlo tree search \cite{chang2005adaptive,kocsis2006bandit}, hyper-parameter tuning \cite{li2016hyperband,jamieson2016non}, discrete optimization \cite{bagaria2018adaptive}, and permutation-based multiple hypothesis testing \cite{zhang2019adaptive}. 
In the context of $k$-medoids clustering, previous work \cite{bagaria2018medoids,baharav2019ultra} has considered finding the \textit{single} medoid of a set points (i.e. the $1$-medoid problem).
In these works, the $1$-medoid problem was also formulated as a best-arm identification problem, with each point being an arm and its average distance to other points being the arm parameter. 
% \mo{Maybe join this last sentence with paragraph below?}

While the $1$-medoid problem considered in prior work can be solved exactly, the $k$-medoids problem is NP-Hard and is therefore only tractable with heuristic solutions. Hence, this paper focuses on improving the computational efficiency of an existing heuristic solution, PAM, that has been empirically observed to be superior to other techniques.
Moreover, instead of having a single best-arm identification problem, we reformulate PAM as a sequence of best-arm problems. We treat different objects as arms in different steps of PAM; in the BUILD step, each point corresponds to an arm, whereas in the SWAP step, each medoid-and-non-medoid pair corresponds to an arm.
% Building on top of this, 
We further notice that the intrinsic difficulties of this sequence of best-arm problems are different, which can be exploited to further speed up the algorithm, as demonstrated in Section \ref{sec:exps} and Appendix \ref{A1}.

% The rest of the paper is organized as follows. In Section \ref{sec:prelims}, we formally describe the $k$-medoids problem and the PAM algorithm. In Section \ref{sec:algo}, we describe our proposed algorithm, \algnamenospace. In section \ref{sec:theory}, we provide theoretical guarantees for when \algname will demonstrate the expected computational speedup while returning the same results and tracking the same optimization trajectory as PAM. In Section \ref{sec:exps}, we experimentally validate \algnamenospace's performance on a number of datasets. We discuss the results of these experiments in Section \ref{sec:discussion}. We conclude with a discussion of future research directions and the broader impact of our work in Sections \ref{sec:future}
%  and \ref{sec:impact}, respectively.